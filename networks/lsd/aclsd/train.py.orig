import json
import logging
import math
import numpy as np
import os
import sys
import torch

from funlib.learn.torch.models import UNet,ConvPass
from gunpowder import *
from gunpowder.ext import torch
from gunpowder.torch import *
from lsd.gp import AddLocalShapeDescriptor

logging.basicConfig(level=logging.INFO)

torch.backends.cudnn.benchmark = True

data_dir = '../../01_data'

samples = [
    'gl0_setup45.zarr',
]

batch_size = 1

neighborhood = [[-1, 0, 0], [0, -1, 0], [0, 0, -1]]

lsd_checkpoint = '../lsd/model_checkpoint_50000'

def calc_max_padding(
        output_size,
        voxel_size,
        neighborhood=None,
        sigma=None,
        mode='shrink'):

    if neighborhood is not None:

        if len(neighborhood) > 3:
            neighborhood = neighborhood[9:12]

        max_affinity = Coordinate(
                [np.abs(aff) for val in neighborhood \
                        for aff in val if aff != 0])

        method_padding = voxel_size * max_affinity

    if sigma:

        method_padding = Coordinate((sigma*3,)*3)

    diag = np.sqrt(output_size[1]**2 + output_size[2]**2)

    max_padding = Roi(
            (Coordinate(
                [i/2 for i in [output_size[0], diag, diag]]) +
                method_padding),
            (0,)*3).snap_to_grid(voxel_size,mode=mode)

    return max_padding.get_begin()


class WeightedMSELoss(torch.nn.MSELoss):

    def __init__(self):
        super(WeightedMSELoss, self).__init__()

    def forward(self,prediction,target,weights):

        scaled = (weights * (prediction - target) ** 2)

        if len(torch.nonzero(scaled)) != 0:
            mask = torch.masked_select(scaled, torch.gt(weights, 0))
            loss = torch.mean(mask)

        else:
            loss = torch.mean(scaled)

        return loss

class ACLSDModel(torch.nn.Module):

    def __init__(
            self,
            in_channels,
            num_fmaps,
            fmap_inc_factor,
            downsample_factors,
            kernel_size_down,
            kernel_size_up,
            constant_upsample=True):

        super().__init__()

        self.unet = UNet(
                in_channels=in_channels,
                num_fmaps=num_fmaps,
                fmap_inc_factor=fmap_inc_factor,
                downsample_factors=downsample_factors,
                kernel_size_down=kernel_size_down,
                kernel_size_up=kernel_size_up,
                constant_upsample=constant_upsample)

        self.affs_head = ConvPass(num_fmaps,3,[[1,1,1]],activation='Sigmoid')

    def forward(self,input):

        z = self.unet(input)

        affs = self.affs_head(z)

        return affs


def train_until(max_iteration):

    downsample = 2

    raw_in_channels = 1
    lsd_in_channels = 10
    num_fmaps = 12
    fmap_inc_factor = 5
    lsd_downsample_factors = [(1,2,2),(1,2,2),(2,2,2)]
    affs_downsample_factors = [(1,2,2),(1,2,2),(2,3,3)]

    kernel_size_down = [
                [(3,)*3, (3,)*3],
                [(3,)*3, (3,)*3],
                [(3,)*3, (3,)*3],
                [(1,3,3), (1,3,3)]]

    kernel_size_up = [
                [(1,3,3), (1,3,3)],
                [(3,)*3, (3,)*3],
                [(3,)*3, (3,)*3]]

    lsd_model = torch.nn.Sequential(
            UNet(
                raw_in_channels,
                num_fmaps,
                fmap_inc_factor,
                lsd_downsample_factors,
                kernel_size_down,
                kernel_size_up,
                constant_upsample=True),
            ConvPass(num_fmaps, 10, [[1]*3], activation='Sigmoid'))

    lsd_model.eval()

    aclsd_model = ACLSDModel(
            lsd_in_channels,
            num_fmaps,
            fmap_inc_factor,
            affs_downsample_factors,
            kernel_size_down,
            kernel_size_up,
            constant_upsample=True)

    loss = WeightedMSELoss()

    optimizer = torch.optim.Adam(
            aclsd_model.parameters(),
            lr=0.5e-4,
            betas=(0.95,0.999))

    raw_fr = ArrayKey('RAW_FR')
    labels_fr = ArrayKey('GT_LABELS_FR')
    labels_mask_fr = ArrayKey('LABELS_MASK_FR')
    unlabelled_fr = ArrayKey('UNLABELLED_FR')

    if downsample > 1:

        raw = ArrayKey('RAW')
        labels = ArrayKey('GT_LABELS')
        labels_mask = ArrayKey('LABELS_MASK')
        unlabelled = ArrayKey('UNLABELLED')

    else:

        raw = ArrayKey('RAW_FR')
        labels = ArrayKey('GT_LABELS_FR')
        labels_mask = ArrayKey('LABELS_MASK_FR')
        unlabelled = ArrayKey('UNLABELLED_FR')

    pred_lsds = ArrayKey('PRED_LSDS')
    gt_affs = ArrayKey('GT_AFFS')
    affs_weights = ArrayKey('AFFS_WEIGHTS')
    affs_mask = ArrayKey('GT_AFFINITIES_MASK')
    pred_affs = ArrayKey('PRED_AFFS')

    input_shape = Coordinate((60,300,300))
    intermediate_shape = Coordinate((40,208,208))
    output_shape = Coordinate((20,84,84))

    voxel_size = Coordinate((40,4*downsample,4*downsample))

    input_size = input_shape * voxel_size
    intermediate_size = intermediate_shape * voxel_size
    output_size = output_shape * voxel_size

    sigma = 80

    labels_padding = calc_max_padding(
            output_size,
            voxel_size,
            sigma=sigma)

    request = BatchRequest()
    request.add(raw, input_size)
    request.add(pred_lsds, intermediate_size)
    request.add(labels, output_size)
    request.add(labels_mask, output_size)
    request.add(unlabelled, output_size)
    request.add(gt_affs, output_size)
    request.add(affs_weights, output_size)
    request.add(affs_mask, output_size)
    request.add(pred_affs, output_size)

    data_sources = tuple(
            ZarrSource(
                    os.path.join(data_dir, sample),
                    {
                        raw_fr: 'volumes/raw',
                        labels_fr: 'volumes/labels/neuron_ids',
                        labels_mask_fr: 'volumes/labels/labels_mask2',
                        unlabelled_fr: 'volumes/labels/unlabeled'
                    },
                    {
                        raw_fr: ArraySpec(interpolatable=True),
                        labels_fr: ArraySpec(interpolatable=False),
                        labels_mask_fr: ArraySpec(interpolatable=False),
                        unlabelled_fr: ArraySpec(interpolatable=False)
                    }
                ) +
            Normalize(raw_fr) +
            Pad(raw_fr, None) +
            Pad(labels_fr, labels_padding) +
            Pad(labels_mask_fr, labels_padding) +
            Pad(unlabelled_fr, labels_padding) +
            RandomLocation() +

            DownSample(raw_fr, (1, downsample, downsample), raw) +
            DownSample(labels_fr, (1, downsample, downsample), labels) +
            DownSample(labels_mask_fr, (1, downsample, downsample), labels_mask) +
            DownSample(unlabelled_fr, (1, downsample, downsample), unlabelled)
            for sample in samples
        )

    train_pipeline = data_sources

    train_pipeline += RandomProvider()

    train_pipeline += ElasticAugment(
            control_point_spacing=[4,int(40/downsample),int(40/downsample)],
            jitter_sigma=[0,2,2],
            rotation_interval=[0,math.pi/2.0],
            prob_slip=0.05,
            prob_shift=0.05,
            max_misalign=int(28/downsample),
            subsample=8)

    train_pipeline += SimpleAugment(transpose_only=[1, 2])

    train_pipeline += IntensityAugment(raw, 0.9, 1.1, -0.1, 0.1, z_section_wise=True)

    train_pipeline += GrowBoundary(
            labels,
            steps=1,
            only_xy=True)

    train_pipeline += AddAffinities(
            neighborhood,
            labels=labels,
            affinities=gt_affs,
            labels_mask=labels_mask,
            unlabelled=unlabelled,
            affinities_mask=affs_mask)

    train_pipeline += BalanceLabels(
            gt_affs,
            affs_weights,
            affs_mask)

    train_pipeline += IntensityScaleShift(raw, 2,-1)

    train_pipeline += Unsqueeze([raw])
    train_pipeline += Stack(batch_size)

    train_pipeline += PreCache(
            cache_size=40,
            num_workers=10)

    train_pipeline += Predict(
        model=lsd_model,
        checkpoint=lsd_checkpoint,
        inputs = {
            'input': raw
        },
        outputs = {
            0: pred_lsds
        })

    train_pipeline += Train(
            model=aclsd_model,
            loss=loss,
            optimizer=optimizer,
            inputs={
                'input': pred_lsds
            },
            loss_inputs={
                0: pred_affs,
                1: gt_affs,
                2: affs_weights
            },
            outputs={
                0: pred_affs
            },
            save_every=5000,
            log_dir='log')

    train_pipeline += Squeeze([raw])
    train_pipeline += Squeeze([raw, pred_lsds, gt_affs, pred_affs])

    train_pipeline += IntensityScaleShift(raw, 0.5, 0.5)

    train_pipeline += Snapshot({
                raw: 'raw',
                labels: 'labels',
                pred_lsds: 'pred_lsds',
                labels_mask: 'labels_mask',
                unlabelled: 'unlabelled',
                gt_affs: 'gt_affs',
                pred_affs: 'pred_affs'
            },
            every=200,
            output_filename='batch_{iteration}.zarr')

    train_pipeline += PrintProfilingStats(every=20)

    with build(train_pipeline) as b:
        for i in range(max_iteration):
            print(i)
            b.request_batch(request)

if __name__ == '__main__':

    iterations = 50000
    train_until(iterations)
